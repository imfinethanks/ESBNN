# Expanding-and-Shrinking Binary Neural Networks

This is the official code of the paper [Xulong Shi, Caiyi Sun, Zhi Qi, Liu Hao, Xiaodong Yang, 2025. Expanding-and-Shrinking Binary Neural Networks. arXiv preprint arXiv:2503.23709.](https://arxiv.org/abs/2503.23709).
### 1.Introduction

While binary neural networks (BNNs) offer significant benefits in terms of speed, memory and energy, they encounter substantial accuracy degradation in challenging tasks compared to their real-valued counterparts. Due to the binarization of weights and activations, the possible values of each entry in the feature maps generated by BNNs are strongly constrained. To tackle this limitation, we propose the expanding-and-shrinking operation, which enhances binary feature maps with negligible increase of computation complexity, thereby strengthening the representation capacity. Extensive experiments conducted on multiple benchmarks reveal that our approach generalizes well across diverse applications ranging from image classification, object detection to generative diffusion model, while also achieving remarkable improvement over various leading binarization algorithms based on different architectures including both CNNs and Transformers.

![Performance](https://raw.githubusercontent.com/imfinethanks/ESBNN/main/png/1.png)
* **Figure**: Overview of performance improvement for image classification (top-1 accuracy) on ImageNet, object detection (mAP) on PASCAL VOC, and generative diffusion model based image super-resolution (PSNR) on Manga. Enabled by the proposed ESBNN, various binary neural networks based on both CNNs and Transformers obtain consistent and significant performance boost.

![ES-BNN](https://raw.githubusercontent.com/imfinethanks/ESBNN/main/png/2.png)
* **Figure**: A schematic overview of the proposed approach ES-BNN utilized in each individual layer of a binary neural network. (a) shows a standard binary convolution layer. (b) illustrates the expanding-and-shrinking operation applied on top of (a). In (c) the binary group convolution is further integrated based on (b).

## 2.Installation

To build the environment for this project on your system, run the following command:
```
sudo pip install -r requirements.txt
```

This will install all the packages listed in the requirements.txt file.


## 3.Run

```
bash src/script/BNext-Tiny/run_distributed_on_disk_a6k5_AdamW_Curicullum_Tiny_assistant_teacher_num_1_aa_rep_ops.sh
```

## 4.Pretrained Models and Comparison With Existing Designs
#### Pretrained Model
|Method |OPs (10^8)|Top-1 Acc  |Pretrained Models| 
|:----:    | :---: | :---:  | :---:               |
|BNext-T| 0.88 |72.4 % |  [BNext-T](https://drive.google.com/file/d/1CJ0XOEhoHuNe-tDYJaAOd1j4YyNXuyas/view?usp=sharing)                  |  
|ES-BNext-T| 0.89 |75.2 (+2.8) % |  [ES-BNext-T](https://pan.seu.edu.cn:443/link/F084D724533B054E64ED1FE9B8CC1459)                  |  

### 4. Reference
If you find our code useful for your research, please cite our paper as follows:
```
@article{xulong2025esbnn,
  title={Expanding-and-Shrinking Binary Neural Networks},
  author={Xulong Shi, Caiyi Sun, Zhi Qi, Liu Hao, Xiaodong Yang},
  journal={arXiv preprint arXiv:2503.23709},
  year={2025}
}
```
